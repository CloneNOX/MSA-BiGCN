{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the jupyter notebook for testing of controversy detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import of default libraries\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Import of the config file\n",
    "from config import config\n",
    "\n",
    "# Setting GPU to run code\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(gpu) for gpu in vars(config)[\"gpu_idx\"])\n",
    "\n",
    "# Import of torch packages\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext import vocab\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Import of self defined classes\n",
    "from DataLoader import DataLoader\n",
    "from Transformer import HierarchicalTransformer\n",
    "from Encoder import WordEncoder\n",
    "from Encoder import PositionEncoder\n",
    "from Optimizer import Optimizer\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.parallel import * \n",
    "\n",
    "# Make some changes to the config\n",
    "config.batch_size_test = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "cpu = torch.device(\"cpu\")\n",
    "split = 0 # Just a hack (Please ignore it)\n",
    "\n",
    "# File paths (For the model's states)\n",
    "folder = \"/home/klingmin/controversy_detection/models/twitter15_split_0/best_model/\"\n",
    "model_file = \"best_model_accuracy_test.pt\"\n",
    "word_encoder_file = \"best_model_word_encoder_accuracy_test.pt\"\n",
    "word_pos_encoder_file = \"best_model_word_pos_encoder_accuracy_test.pt\"\n",
    "\n",
    "model_path = os.path.join(folder, model_file)\n",
    "word_encoder_path = os.path.join(folder, word_encoder_file)\n",
    "word_pos_encoder_path = os.path.join(folder, word_pos_encoder_file)\n",
    "\n",
    "# File paths (For the test file)\n",
    "test_file_path = \"../../data/processed/test.json\"\n",
    "\n",
    "# File path (For glove)\n",
    "glove_directory = \"../../data/glove/\"\n",
    "glove_file = \"glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mappings of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string_to_id = {\"false\" : 0,\n",
    "                \"true\" : 1,\n",
    "                \"unverified\" : 2,\n",
    "                \"non-rumor\" : 3}\n",
    "\n",
    "\n",
    "id_to_string = {0 : \"false\",\n",
    "                1 : \"true\",\n",
    "                2 : \"unverified\",\n",
    "                3 : \"non-rumor\"}\n",
    "\n",
    "\n",
    "mapping_for_predicted_y = {\"false\" : \"false\",\n",
    "                           \"true\" : \"true\",\n",
    "                           \"unverified\" : \"unverified\",\n",
    "                           \"non-rumor\" : \"non-rumor\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new state dict - Issues because of multi-GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_new_state_dict(current_state_dict):\n",
    "    \n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in current_state_dict.items():\n",
    "        name = k[7:] # remove module.\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_labels(tensor):\n",
    "    \n",
    "    return np.argmax(tensor, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id_ index to id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id_index_to_id_(id_list, loader):\n",
    "    \n",
    "    id_index = [loader.id_field.vocab.itos[id_] for id_ in id_list]\n",
    "    return id_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the saved states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading in the saved states \n",
    "model_checkpoint = create_new_state_dict(torch.load(model_path))\n",
    "word_encoder_checkpoint = torch.load(word_encoder_path)\n",
    "word_pos_encoder_checkpoint = torch.load(word_pos_encoder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved states into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing RD for chinese\n"
     ]
    }
   ],
   "source": [
    "# Load states into the model\n",
    "hierarchical_transformer = HierarchicalTransformer.HierarchicalTransformer(config)\n",
    "hierarchical_transformer.load_state_dict(model_checkpoint)\n",
    "hierarchical_transformer.eval()\n",
    "\n",
    "# Getting the data loader\n",
    "loader = DataLoader.DataLoader(config, split, type_ = \"test\", lang = \"zh\")\n",
    "loader.define_fields()\n",
    "loader.test = loader.read_data(test_file_path)\n",
    "\n",
    "# Getting the vocab vectors\n",
    "vec = vocab.Vectors(name = glove_file, cache = glove_directory)\n",
    "\n",
    "# Building the id_field\n",
    "loader.id_field.build_vocab(getattr(loader.test, config.keys_order[\"post_id\"]))\n",
    "\n",
    "# Build the vocabulary (for tweets) using the test dataset\n",
    "loader.tweet_field.build_vocab(getattr(loader.test, config.keys_order[\"content\"]), \n",
    "                               max_size = config.max_vocab, \n",
    "                               vectors = vec)\n",
    "\n",
    "# Iterating through the test set to get test batches \n",
    "loader.test_batch = loader.load_batches(loader.test, config.batch_size_test)\n",
    "\n",
    "# Getting the word encoder\n",
    "word_encoder = WordEncoder.WordEncoder(config, loader)\n",
    "word_encoder.eval()\n",
    "\n",
    "# Getting the word position encoder \n",
    "word_pos_encoder = PositionEncoder.PositionEncoder(config, config.max_length)\n",
    "word_pos_encoder.load_state_dict(word_pos_encoder_checkpoint)\n",
    "word_pos_encoder.eval()\n",
    "\n",
    "# Getting the time delay encoder\n",
    "time_delay_encoder = PositionEncoder.PositionEncoder(config, config.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving the modules to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving the model to the GPU\n"
     ]
    }
   ],
   "source": [
    "if config.gpu:\n",
    "    print(\"Moving the model to the GPU\")\n",
    "    if len(config.gpu_idx) > 1:\n",
    "        hierarchical_transformer = DataParallelModel(hierarchical_transformer.cuda())\n",
    "    else:\n",
    "        hierarchical_transformer = hierarchical_transformer.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting of test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for id_, X, y, word_pos, time_delay, structure, attention_mask_word, attention_mask_post in loader.get_data(\"test\", return_id = True):\n",
    "\n",
    "        # <-------------- Casting as a variable -------------->\n",
    "        id_ = id_index_to_id_(id_, loader)\n",
    "        X = Variable(X)\n",
    "        X_ = X\n",
    "        word_pos = Variable(word_pos)\n",
    "        time_delay = Variable(time_delay)\n",
    "        structure = Variable(structure)\n",
    "        attention_mask_word = Variable(attention_mask_word)\n",
    "        attention_mask_post = Variable(attention_mask_post)\n",
    "        \n",
    "        # <-------------- Encode content -------------->\n",
    "        X = word_encoder(X)\n",
    "        word_pos = word_pos_encoder(word_pos)\n",
    "        time_delay = time_delay_encoder(time_delay)\n",
    "\n",
    "        # <-------------- Move to GPU -------------->\n",
    "        if config.gpu:\n",
    "            X = X.cuda()\n",
    "            word_pos = word_pos.cuda()\n",
    "            time_delay = time_delay.cuda()\n",
    "            structure = structure.cuda()\n",
    "            attention_mask_word = attention_mask_word.cuda()\n",
    "            attention_mask_post = attention_mask_post.cuda()\n",
    "\n",
    "        # <-------------- Getting the predictions -------------->\n",
    "        if len(config.gpu_idx) > 1:\n",
    "            \n",
    "            predicted_y, last_layer_attention, self_atten_weights_dict_word, self_atten_weights_dict_post = zip(* hierarchical_transformer(X, word_pos, time_delay, structure, attention_mask_word = attention_mask_word, attention_mask_post = attention_mask_post, return_attention = True))\n",
    "\n",
    "            # Merge into 1 batch \n",
    "            predicted_y = torch.cat(list(predicted_y), dim = 0)\n",
    "            last_layer_attention = torch.cat(list(last_layer_attention), dim = 0)\n",
    "\n",
    "            self_atten_weights_dict_word = merge_attention_dict(self_atten_weights_dict_word, config, \"word\")\n",
    "            self_atten_weights_dict_post = merge_attention_dict(self_atten_weights_dict_post, config, \"post\")\n",
    "\n",
    "        else:\n",
    "            predicted_y, last_layer_attention, self_atten_weights_dict_word, self_atten_weights_dict_post = hierarchical_transformer(X, word_pos, time_delay, structure, attention_mask_word = attention_mask_word, attention_mask_post = attention_mask_post, return_attention = True)\n",
    "\n",
    "        \n",
    "        # <-------------- Getting the predicted labels -------------->\n",
    "        predicted_y = predicted_y.cpu().numpy()\n",
    "        predicted_y_labels = get_labels(predicted_y)\n",
    "        predicted_y_labels = [id_to_string[y] for y in predicted_y_labels]\n",
    "        \n",
    "        # <-------------- Getting the predicted labels -------------->\n",
    "        current_results = {id_[i] : {\"predicted_y\" : mapping_for_predicted_y[predicted_y_labels[i]], \"true_y\" : id_to_string[y.cpu().numpy()[i]]} for i in range(len(predicted_y_labels))}\n",
    "        \n",
    "        # <-------------- Merging the results -------------->\n",
    "        results_dict = {**results_dict, **current_results}\n",
    "        \n",
    "        # <-------------- Free up the GPU -------------->\n",
    "        del id_\n",
    "        del X\n",
    "        del y\n",
    "        del predicted_y\n",
    "        del word_pos\n",
    "        del time_delay\n",
    "        del structure\n",
    "        del attention_mask_word\n",
    "        del attention_mask_post\n",
    "        del self_atten_weights_dict_word\n",
    "        del self_atten_weights_dict_post\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting statistics of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the breakdown of the predicted labels : Counter({'true': 6, 'false': 4})\n",
      "This is the accuracy of the model : 60.0%\n",
      "This is the F-score of the model : 58.3%\n"
     ]
    }
   ],
   "source": [
    "breakdown = collections.Counter([record[\"predicted_y\"] for record in results_dict.values()])\n",
    "acc = sum([record[\"predicted_y\"] == record[\"true_y\"] for record in results_dict.values()]) / len(results_dict) * 100\n",
    "\n",
    "true_y = [record[\"true_y\"] for record in results_dict.values()]\n",
    "pred_y = [record[\"predicted_y\"] for record in results_dict.values()]\n",
    "pre, recall, fscore, _ = precision_recall_fscore_support(true_y, pred_y, average = \"macro\")\n",
    "fscore = round(fscore * 100, 1)\n",
    "\n",
    "print(\"This is the breakdown of the predicted labels : {}\".format(breakdown))\n",
    "print(\"This is the accuracy of the model : {}%\".format(acc))\n",
    "print(\"This is the F-score of the model : {}%\".format(fscore))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
