{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_index(path):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for line in open(path, \"r\", encoding = \"UTF-8\"):\n",
    "        \n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        data.append(line)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for line in open(path, \"r\", encoding = \"UTF-8\"):\n",
    "        data.append(json.loads(line))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(full_data, index):\n",
    "    \n",
    "    sieved_data = []\n",
    "    \n",
    "    for item in full_data:\n",
    "        \n",
    "        if item[\"id_\"] in index:\n",
    "            sieved_data.append(item)\n",
    "    \n",
    "    return sieved_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map time delay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_time_bins(time):\n",
    "    \n",
    "    # Max is size -1 (Ie, max index is 99 if i set size to be 100) so I would have 100 unique index\n",
    "    \n",
    "    bin_num = min(int(time // float(INTERVAL)), SIZE - 1)\n",
    "    bin_num = max(0, bin_num)\n",
    "    \n",
    "    return bin_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_small_data(data, num_records = 32, num_tweets = 5):\n",
    "    \n",
    "    small_data = data[:num_records]\n",
    "    \n",
    "    for i in range(len(small_data)):\n",
    "        \n",
    "        small_data[i][\"tweets\"] = small_data[i][\"tweets\"][:num_tweets]\n",
    "        small_data[i][\"time_delay\"] = small_data[i][\"time_delay\"][:num_tweets]\n",
    "    \n",
    "    return small_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get time bins of unique posts in a claim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_posts_time_bin(data):\n",
    "    \n",
    "    new_data = []\n",
    "    new_data_tweets_length_lst = []\n",
    "    \n",
    "    for record in data:\n",
    "        \n",
    "        time_delay = record[\"time_delay\"]\n",
    "        tweet_ids = record[\"tweet_ids\"]\n",
    "        tweets = record[\"tweets\"]\n",
    "        num_tweets = len(tweets)\n",
    "        \n",
    "        time_delay_new = []\n",
    "        tweets_new = []\n",
    "        \n",
    "        time_delay_keys = list(set(time_delay))\n",
    "        \n",
    "        for key in time_delay_keys:\n",
    "            \n",
    "            idx = [i for i in range(num_tweets) if time_delay[i] == key]\n",
    "            unique_tweet_ids_current = list(set([tweet_ids[i] for i in idx]))\n",
    "            \n",
    "            for id_ in unique_tweet_ids_current:\n",
    "                \n",
    "                tweets_new.append(tweets[tweet_ids.index(id_)])\n",
    "                time_delay_new.append(key)\n",
    "        \n",
    "        new_data.append({\"id_\" : record[\"id_\"],\n",
    "                         \"label\" : record[\"label\"],\n",
    "                         \"tweets\" : tweets_new, \n",
    "                         \"time_delay\" : time_delay_new\n",
    "                        })\n",
    "        \n",
    "        new_data_tweets_length_lst.append(len(tweets_new))\n",
    "    \n",
    "    return new_data, new_data_tweets_length_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unique posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_posts(data):\n",
    "    \n",
    "    new_data = []\n",
    "    new_data_tweets_length_lst = []\n",
    "\n",
    "    for record in data:\n",
    "        \n",
    "        new_time_delay = []\n",
    "        new_tweet_ids = []\n",
    "        new_tweets = []\n",
    "        \n",
    "        source_claim = record[\"claim\"]\n",
    "        time_delay = record[\"time_delay\"]\n",
    "        tweet_ids = record[\"tweet_ids\"]\n",
    "        tweets = record[\"tweets\"]\n",
    "\n",
    "        unique_tweet_ids = list(set(tweet_ids))\n",
    "        unique_tweet_ids = [id_ for id_ in unique_tweet_ids if id_ != source_claim[\"tweet_id\"].strip()]\n",
    "        \n",
    "        for unique_id in unique_tweet_ids:\n",
    "            \n",
    "            idx = tweet_ids.index(unique_id)\n",
    "            tweet = tweets[idx]\n",
    "            relevant_time = [time_delay[i] for i in range(len(time_delay)) if tweet_ids[i] == unique_id]\n",
    "            min_time = min(relevant_time)\n",
    "            \n",
    "            new_time_delay.append(min_time)\n",
    "            new_tweet_ids.append(unique_id)\n",
    "            new_tweets.append(tweet)\n",
    "        \n",
    "        # <--------- Sort according to time delay --------->\n",
    "        sort_order = list(map(operator.itemgetter(0), sorted(enumerate(new_time_delay), key=operator.itemgetter(1))))\n",
    "        \n",
    "        new_time_delay = [new_time_delay[i] for i in sort_order]\n",
    "        new_tweet_ids = [new_tweet_ids[i] for i in sort_order]\n",
    "        new_tweets = [new_tweets[i] for i in sort_order]\n",
    "        \n",
    "        # <--------- Add in the source post --------->\n",
    "        new_time_delay = [float(source_claim[\"time_delay\"])] + new_time_delay\n",
    "        new_time_delay = [map_time_bins(time) for time in new_time_delay]\n",
    "        \n",
    "        new_tweet_ids = [source_claim[\"tweet_id\"]] + new_tweet_ids\n",
    "        new_tweets = [source_claim[\"tweet\"]] + new_tweets\n",
    "        \n",
    "        new_data.append({\"id_\" : record[\"id_\"],\n",
    "                         \"label\" : record[\"label\"],\n",
    "                         \"tweets\" : new_tweets, \n",
    "                         \"time_delay\" : new_time_delay\n",
    "                        })\n",
    "\n",
    "        new_data_tweets_length_lst.append(len(new_tweets))\n",
    "    \n",
    "    return new_data, new_data_tweets_length_lst\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change label (Claims with only the source claim would be now unverified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_labels(data):\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        record = data[i]\n",
    "        \n",
    "        if len(record[\"time_delay\"]) == 1 and record[\"label\"] != 2:\n",
    "            \n",
    "            new_record = record\n",
    "            new_record[\"label\"] = 2 # Set as unverified \n",
    "            data[i] = new_record\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_test_data(data):\n",
    "    \n",
    "    idx = []\n",
    "    label = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        idx.append(i)\n",
    "        label.append(data[i][\"label\"])\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.5, random_state = 0)\n",
    "    \n",
    "    for idx_1, idx_2 in sss.split(idx, label):\n",
    "        \n",
    "        label_1 = [label[i] for i in idx_1]\n",
    "        label_2 = [label[i] for i in idx_2]\n",
    "        \n",
    "        data_1 = [data[i] for i in idx_1]\n",
    "        data_2 = [data[i] for i in idx_2]\n",
    "        \n",
    "        return data_1, data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_data(data, path):\n",
    "    \n",
    "    with open(path, \"w\", encoding = \"UTF-8\") as f:\n",
    "        \n",
    "        for item in data:\n",
    "            \n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(data, split_num, year):\n",
    "    \n",
    "    # Input file paths\n",
    "    train_index_path = glob.glob(\"../data/controversy/raw_data/split/*trainSet_Twitter{}_{}*\".format(year, split_num))[0]\n",
    "    test_index_path = glob.glob(\"../data/controversy/raw_data/split/*testSet_Twitter{}_{}*\".format(year, split_num))[0]\n",
    "    \n",
    "    # Output file paths\n",
    "    train_data_output_path = \"../data/controversy/processed_data/linear_structure/twitter{}/split_data/split_{}/train_unique.json\".format(year, split_num)\n",
    "    test_data_1_output_path = \"../data/controversy/processed_data/linear_structure/twitter{}/split_data/split_{}/test_1_unique.json\".format(year, split_num)\n",
    "    test_data_2_output_path = \"../data/controversy/processed_data/linear_structure/twitter{}/split_data/split_{}/test_2_unique.json\".format(year, split_num)\n",
    "    \n",
    "    # Output file paths (small)\n",
    "    train_data_output_small_path = \"../data/controversy/processed_data/linear_structure/twitter{}/split_data/split_{}/train_small_unique.json\".format(year, split_num)\n",
    "    test_data_1_output_small_path = \"../data/controversy/processed_data/linear_structure/twitter{}/split_data/split_{}/test_1_small_unique.json\".format(year, split_num)\n",
    "    test_data_2_output_small_path = \"../data/controversy/processed_data/linear_structure/twitter{}/split_data/split_{}/test_2_small_unique.json\".format(year, split_num)\n",
    "\n",
    "    # Read data\n",
    "    train_index = read_index(train_index_path)\n",
    "    test_index = read_index(test_index_path)\n",
    "    \n",
    "    # get_data \n",
    "    train_data = get_data(data, train_index)\n",
    "    test_data = get_data(data, test_index)\n",
    "    \n",
    "    # Get unique posts \n",
    "    train_data_unique, train_length_lst = get_unique_posts(train_data)\n",
    "    test_data_unique, test_length_lst = get_unique_posts(test_data)\n",
    "    \n",
    "    print(\"Max training is: {}, Max testing is: {}\".format(max(train_length_lst), min(train_length_lst)))\n",
    "    print(\"Min training is: {}, Min testing is: {}\".format(max(test_length_lst), min(test_length_lst)))\n",
    "    print()\n",
    "    \n",
    "    # Changing the labels of the data \n",
    "    train_data_unique = change_labels(train_data_unique)\n",
    "    test_data_unique = change_labels(test_data_unique)\n",
    "        \n",
    "    print(\"Training labels: {}\".format(collections.Counter([item[\"label\"] for item in train_data_unique])))\n",
    "    print(\"Testing labels: {}\".format(collections.Counter([item[\"label\"] for item in test_data_unique])))\n",
    "    print()\n",
    "\n",
    "    # Split test_data into 2 sets \n",
    "    test_data_1, test_data_2 = split_test_data(test_data_unique)\n",
    "    \n",
    "    # write data\n",
    "    write_data(train_data_unique, train_data_output_path)\n",
    "    write_data(test_data_1, test_data_1_output_path)\n",
    "    write_data(test_data_2, test_data_2_output_path)\n",
    "    \n",
    "    # write data (small)\n",
    "    write_data(get_small_data(train_data_unique), train_data_output_small_path)\n",
    "    write_data(get_small_data(test_data_1), test_data_1_output_small_path)\n",
    "    write_data(get_small_data(test_data_2), test_data_2_output_small_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Settings\n",
    "    SIZE = 100\n",
    "    INTERVAL = 10\n",
    "    years = [15, 16]\n",
    "    \n",
    "    for year in tqdm(years):\n",
    "        \n",
    "        compiled_data_path = \"../data/controversy/processed_data/linear_structure/twitter{}/full_data/compiled_data.json\".format(year)\n",
    "        compiled_data = read_data(compiled_data_path)\n",
    "        \n",
    "        for i in tqdm(range(5)):\n",
    "            print(\"Doing split {} for year {}\".format(i, year))\n",
    "            main(compiled_data, i, year)\n",
    "            print(\"*\" * 100)\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
