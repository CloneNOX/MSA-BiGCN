{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "dataset = semEval2017Dataset(\n",
    "    dataPath='../dataset/semeval2017-task8/', \n",
    "    type='train'\n",
    ")\n",
    "glove25d = KeyedVectors.load_word2vec_format(\n",
    "    '../dataset/glove/glove.twitter.27B.25d.gensim.txt',\n",
    "    binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorSize = glove25d.vector_size\n",
    "glove25d.add_vectors([\"<start>\", \"<end>\", \"<unk>\"] ,np.random.randn(3, vectorSize))\n",
    "with open('../dataset/semeval2017-task8/wordList.json', 'r') as f:\n",
    "    content = f.read()\n",
    "wordList = [\"<unk>\", \"<start>\", \"<end>\"]\n",
    "wordList += (json.loads(content)).keys()\n",
    "word2index = {}\n",
    "index = 1\n",
    "for word in wordList:\n",
    "    if word in glove25d:\n",
    "        word2index[word] = index\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data import collate\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle = True,\n",
    "    num_workers = 4,\n",
    "    collate_fn = collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ABGCN import *\n",
    "from torch import optim\n",
    "model = ABGCN(\n",
    "    word2vec = glove25d,\n",
    "    word2index = word2index,\n",
    "    s2vDim = 64, # 使用的句嵌入的维度\n",
    "    gcnHiddenDim = 64, # GCN隐藏层的维度（GCNconv1的输出维度）\n",
    "    rumorFeatureDim = 64, # GCN输出层的维度\n",
    "    numRumorTag = 3, # 谣言标签种类数\n",
    "    numStanceTag = 4, # 立场标签种类数\n",
    "    numHeads = 5\n",
    ")\n",
    "device = torch.device('cpu')\n",
    "model = model.set_device(device)\n",
    "loss_func = torch.nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9258, grad_fn=<DivBackward0>) 0.6873525247758376 0.32475975889635295\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "task = 2\n",
    "true = []\n",
    "pre = []\n",
    "totalLoss = 0.\n",
    "for epoch in range(1, 2):\n",
    "    model.train()\n",
    "    for thread in loader:\n",
    "        if task == 1:\n",
    "            tag = thread['rumorTag'].to(device)\n",
    "            true += thread['rumorTag'].tolist()\n",
    "        else:\n",
    "            tag = thread['stanceTag'].to(device)\n",
    "            true += thread['stanceTag'].tolist()\n",
    "        \n",
    "        \n",
    "        nodeText = thread['nodeText']\n",
    "        for i in range(len(nodeText)):\n",
    "            indexList = []\n",
    "            for word in nodeText[i]:\n",
    "                if word in word2index:\n",
    "                    indexList.append(word2index[word])\n",
    "                elif word != '':\n",
    "                    indexList.append(word2index['<unk>'])\n",
    "            nodeText[i] = torch.IntTensor(indexList).to(device)\n",
    "        nodeText = pad_sequence(nodeText, padding_value=0, batch_first=True)\n",
    "        thread['nodeText'] = nodeText\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predict = model.forward(thread, task)\n",
    "        loss = loss_func(predict, tag)\n",
    "        totalLoss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predict = softmax(predict, dim=1)\n",
    "        pre += predict.max(dim=1)[1].tolist()\n",
    "    macroF1 = f1_score(true, pre, labels=[0,1,2,3], average='macro')\n",
    "    acc = (np.array(true) == np.array(pre)).sum() / len(true)\n",
    "    print(totalLoss / len(loader), acc, macroF1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53024653744be1c7fdb50f623c21bc7cbc066eeadd60abd98245bb69a9f2fbe7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('xsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
