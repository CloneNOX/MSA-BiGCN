{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from utils import flattenStructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "dataset = semeval2017Dataset(dataPath='../dataset/semeval2017-task8/', \n",
    "                             type='train',\n",
    "                             w2vPath='../dataset/glove/',\n",
    "                             w2vDim=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from biGCN import *\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outDim, dropout=0.):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(inputDim, hiddenDim)\n",
    "        self.conv2 = GCNConv(hiddenDim + inputDim, outDim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        posts, edge_index, rootIndex = data.x, data.edgeIndex, data.rootIndex # posts(n, inputDim), edgeIndex(2, |E|)\n",
    "        \n",
    "        conv1Out = self.conv1(posts, edge_index)\n",
    "        postRoot = torch.clone(posts[rootIndex])\n",
    "        postRoot = postRoot.repeat(posts.shape[0], 1)\n",
    "        conv1Root = conv1Out[rootIndex]\n",
    "\n",
    "        conv2In = torch.cat([conv1Out, postRoot], dim=1)\n",
    "        conv2In = F.relu(conv2In)\n",
    "        conv2In = F.dropout(conv2In, training=self.training) # BiGCN对于dropout的实现，一次卷积之后随机舍弃一些点\n",
    "        conv2Out = self.conv2(conv2In, edge_index)\n",
    "        conv2Out = F.relu(conv2Out)\n",
    "\n",
    "        conv1Root = conv1Root.repeat(posts.shape[0], 1)\n",
    "        feature = torch.cat([conv1Root, conv2Out], dim=1)\n",
    "        # 使用均值计算，把所有节点的特征聚合成为图的特征\n",
    "        feature = torch.mean(feature, dim=0).view(1, -1)\n",
    "        return feature\n",
    "    \n",
    "    # 更换计算设备\n",
    "    def set_device(self, device: torch.device) -> torch.nn.Module:\n",
    "        _model = self.to(device)\n",
    "        _model.device = device\n",
    "        return _model\n",
    "    # 保存模型\n",
    "    def save(self, path: str):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    # 加载模型\n",
    "    def load(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "class BiGCN(torch.nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, convOutDim, NumRumorTag):\n",
    "        super(BiGCN, self).__init__()\n",
    "        self.TDGCN = GCN(inputDim, hiddenDim, convOutDim)\n",
    "        self.BUGCN = GCN(inputDim, hiddenDim, convOutDim)\n",
    "        self.fc=torch.nn.Linear((convOutDim + hiddenDim) * 2, NumRumorTag)\n",
    "\n",
    "    def forward(self, dataTD, dataBU):\n",
    "        TDOut = self.TDGCN(dataTD)\n",
    "        BUOut = self.BUGCN(dataBU)\n",
    "        feature = torch.cat((TDOut, BUOut), dim=1)\n",
    "        p = self.fc(feature)\n",
    "        return p\n",
    "\n",
    "    # 更换计算设备\n",
    "    def set_device(self, device: torch.device) -> torch.nn.Module:\n",
    "        _model = self.to(device)\n",
    "        _model.device = device\n",
    "        return _model\n",
    "    # 保存模型\n",
    "    def save(self, path: str):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    # 加载模型\n",
    "    def load(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "class ABGCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 w2vDim: int, # 使用的词嵌入的维度\n",
    "                 s2vDim: int, # 使用的句嵌入的维度\n",
    "                 gcnHiddenDim: int, # GCN隐藏层的维度（GCNconv1的输出维度）\n",
    "                 rumorFeatureDim: int, # GCN输出层的维度\n",
    "                 numRumorTag: int, # 谣言标签种类数\n",
    "                 numStanceTag: int, # 立场标签种类数\n",
    "                 s2vMethon = 'a', # 获取据嵌入的方法（l:lstm; a:attention）\n",
    "                 numLstmLayer = 1 # lstm层数，仅在s2vMethod == 'l'时有效\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w2vDim = w2vDim\n",
    "        self.s2vDim = s2vDim\n",
    "        self.gcnHiddenDim = gcnHiddenDim\n",
    "        self.rumorFeatureDim = rumorFeatureDim\n",
    "        self.s2vMethon = s2vMethon\n",
    "        self.batchSize = 1 # 实际上，由于不会写支持batch化的GCN，模型时不支持batch化训练的\n",
    "        self.numRumorTag = numRumorTag\n",
    "        self.numStanceTag = numStanceTag\n",
    "        self.device = 'cpu'\n",
    "\n",
    "        # ==使用biLSTM获取post的向量表示==\n",
    "        if self.s2vMethon == 'l':\n",
    "            self.numLstmLayer = numLstmLayer\n",
    "            self.lstm = nn.LSTM(input_size = self.w2vDim,\n",
    "                                hidden_size = self.s2vDim,\n",
    "                                num_layers = self.numLstmLayer,\n",
    "                                bidirectional=True)\n",
    "            self.h0 = nn.Parameter(torch.randn((self.numLstmLayer * 2, self.batchSize, self.s2vDim)))\n",
    "            self.c0 = nn.Parameter(torch.randn((self.numLstmLayer * 2, self.batchSize, self.s2vDim)))\n",
    "            self.s2vDim *= 2 # 由于使用BiDirect所以s2vDim的维度会扩大1倍\n",
    "        # ==使用Attention==\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        self.biGCN = BiGCN(self.s2vDim, self.gcnHiddenDim, self.rumorFeatureDim, self.numRumorTag)\n",
    "\n",
    "    def forwardRumor(self, data):\n",
    "        # 各节点的特征表示，此时是word2vec的形式\n",
    "        nodeFeature = data['nodeFeature']\n",
    "\n",
    "        # 把w2v转化成s2v作为节点的特征\n",
    "        s2v = []\n",
    "        if self.s2vMethon == 'l':\n",
    "            for w2v in nodeFeature:\n",
    "                w2v = w2v.view((len(w2v), 1, -1)).to(self.device)\n",
    "                sentenceHidden, _ = self.lstm(w2v, (self.h0, self.c0))\n",
    "                # 仅取出最后一层的隐状态作为s2v\n",
    "                s2v.append(sentenceHidden[-1].view(1, len(w2v[-1]), -1))\n",
    "            s2v = torch.cat(s2v, dim=0)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # GCN处理\n",
    "        s2v = s2v.view(s2v.shape[0], -1)\n",
    "        dataTD = Data(x = s2v.to(self.device), \n",
    "                      edgeIndex = data['edgeIndexTD'].to(self.device), \n",
    "                      rootIndex = data['threadIndex'])\n",
    "        dataBU = Data(x = s2v.to(self.device), \n",
    "                      edgeIndex = data['edgeIndexBU'].to(self.device), \n",
    "                      rootIndex = data['threadIndex'])\n",
    "        p = self.biGCN(dataTD, dataBU).view(self.batchSize, -1) # p.shape = (1, *)\n",
    "        \n",
    "        return p\n",
    "        \n",
    "\n",
    "    # 更换计算设备\n",
    "    def set_device(self, device: torch.device) -> torch.nn.Module:\n",
    "        _model = self.to(device)\n",
    "        _model.device = device\n",
    "        return _model\n",
    "    # 保存模型\n",
    "    def save(self, path: str):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    # 加载模型\n",
    "    def load(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ABGCN import *\n",
    "from torch import optim\n",
    "model = ABGCN(w2vDim = 25,\n",
    "              s2vDim = 128,\n",
    "              gcnHiddenDim = 128,\n",
    "              rumorFeatureDim = 128,\n",
    "              numRumorTag = 3,\n",
    "              numStanceTag = 4,\n",
    "              s2vMethon = 'l')\n",
    "device = torch.device('cuda')\n",
    "model = model.set_device(device)\n",
    "loss_func = torch.nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-2, momentum=0.9)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "totalLoss = 0.\n",
    "testdata = dataset[0]\n",
    "for data in tqdm(iter(loader), \n",
    "                 desc=\"[epoch {:d}, rumor]\".format(1),\n",
    "                 leave=False, \n",
    "                 ncols=100):\n",
    "    # 抹除dataloader生成batch时对数据的升维\n",
    "    data['threadId'] = data['threadId'][0]\n",
    "    data['threadIndex'] = data['threadIndex'][0]\n",
    "    for i in range(len(data['nodeFeature'])):\n",
    "        data['nodeFeature'][i] = data['nodeFeature'][i].view((data['nodeFeature'][i].shape[1], data['nodeFeature'][i].shape[2]))\n",
    "    data['edgeIndexTD'] = data['edgeIndexTD'].view((data['edgeIndexTD'].shape[1], data['edgeIndexTD'].shape[2]))\n",
    "    data['edgeIndexBU'] = data['edgeIndexBU'].view((data['edgeIndexBU'].shape[1], data['edgeIndexBU'].shape[2]))\n",
    "    data['rumorTag'] = data['rumorTag'].view((data['rumorTag'].shape[1]))\n",
    "    data['stanceTag'] = data['stanceTag'].view((data['stanceTag'].shape[1]))\n",
    "    rumorTag = data['rumorTag'].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    p = model.forwardRumor(data)\n",
    "    loss = loss_func(p, rumorTag)\n",
    "    totalLoss += loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    p = softmax(p, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(280.0627, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(totalLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BiGCN' object has no attribute 'feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1972043/709165378.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiGCN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/xsr/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BiGCN' object has no attribute 'feature'"
     ]
    }
   ],
   "source": [
    "print(model.biGCN.feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9175, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53024653744be1c7fdb50f623c21bc7cbc066eeadd60abd98245bb69a9f2fbe7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('xsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
