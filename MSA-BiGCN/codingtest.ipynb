{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "dataset = semEval2017Dataset(\n",
    "    dataPath='../dataset/semeval2017-task8/', \n",
    "    type='train'\n",
    ")\n",
    "glove25d = KeyedVectors.load_word2vec_format(\n",
    "    '../dataset/glove/glove.twitter.27B.25d.gensim.txt',\n",
    "    binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorSize = glove25d.vector_size\n",
    "glove25d.add_vectors([\"<start>\", \"<end>\", \"<unk>\"] ,np.random.randn(3, vectorSize))\n",
    "with open('../dataset/semeval2017-task8/wordList.json', 'r') as f:\n",
    "    content = f.read()\n",
    "wordList = [\"<unk>\", \"<start>\", \"<end>\"]\n",
    "wordList += (json.loads(content)).keys()\n",
    "word2index = {}\n",
    "index = 1\n",
    "for word in wordList:\n",
    "    if word in glove25d:\n",
    "        word2index[word] = index\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data import collate\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle = True,\n",
    "    num_workers = 4,\n",
    "    collate_fn = collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ABGCN import *\n",
    "from torch import optim\n",
    "model = ABGCN(\n",
    "    word2vec = glove25d,\n",
    "    word2index = word2index,\n",
    "    s2vDim = 32, # 使用的句嵌入的维度\n",
    "    gcnHiddenDim = 64, # GCN隐藏层的维度（GCNconv1的输出维度）\n",
    "    rumorFeatureDim = 64, # GCN输出层的维度\n",
    "    numRumorTag = 3, # 谣言标签种类数\n",
    "    numStanceTag = 4, # 立场标签种类数\n",
    ")\n",
    "device = torch.device('cuda')\n",
    "model = model.set_device(device)\n",
    "loss_func = torch.nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9632352941176471 0.9580224383994463\n",
      "0.9145823501651722 0.8577120638232192\n",
      "tensor(0.4296, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rumorTrue = []\n",
    "rumorPre = []\n",
    "stanceTrue = []\n",
    "stancePre = []\n",
    "totalLoss = 0.\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    model.train()\n",
    "    for thread in loader:\n",
    "        rumorTag = thread['rumorTag'].to(device)\n",
    "        rumorTrue += thread['rumorTag'].tolist()\n",
    "        stanceTag = thread['stanceTag'].to(device)\n",
    "        stanceTrue += thread['stanceTag'].tolist()\n",
    "        \n",
    "        nodeText = thread['nodeText']\n",
    "        for i in range(len(nodeText)):\n",
    "            indexList = []\n",
    "            for word in nodeText[i]:\n",
    "                if word in word2index:\n",
    "                    indexList.append(word2index[word])\n",
    "                elif word != '':\n",
    "                    indexList.append(word2index['<unk>'])\n",
    "            nodeText[i] = torch.IntTensor(indexList).to(device)\n",
    "        nodeText = pad_sequence(nodeText, padding_value=0, batch_first=True)\n",
    "        thread['nodeText'] = nodeText\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        rumorPredict, stancePredict = model.forward(thread)\n",
    "        loss = loss_func(rumorPredict, rumorTag) + loss_func(stancePredict, stanceTag)\n",
    "        totalLoss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        rumorPredict = softmax(rumorPredict, dim=1)\n",
    "        rumorPre += rumorPredict.max(dim=1)[1].tolist()\n",
    "        stancePredict = softmax(stancePredict, dim=1)\n",
    "        stancePre += stancePredict.max(dim=1)[1].tolist()\n",
    "    macroF1 = f1_score(rumorTrue, rumorPre, labels=[0,1,2], average='macro')\n",
    "    acc = (np.array(rumorTrue) == np.array(rumorPre)).sum() / len(rumorTrue)\n",
    "    print(acc, macroF1)\n",
    "    macroF1 = f1_score(stanceTrue, stancePre, labels=[0,1,2,3], average='macro')\n",
    "    acc = (np.array(stanceTrue) == np.array(stancePre)).sum() / len(stanceTrue)\n",
    "    print(acc, macroF1)\n",
    "    print(totalLoss / len(loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class ABGCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        word2vec, # 预训练词向量\n",
    "        word2index, # 词-下表映射\n",
    "        s2vDim: int, # 使用的句嵌入的维度\n",
    "        gcnHiddenDim: int, # GCN隐藏层的维度（GCNconv1的输出维度）\n",
    "        rumorFeatureDim: int, # GCN输出层的维度\n",
    "        numRumorTag: int, # 谣言标签种类数\n",
    "        numStanceTag: int, # 立场标签种类数\n",
    "        w2vAttentionHeads = 5, # multi-head attention中使用的头数\n",
    "        s2vAttentionHeads = 8,\n",
    "        needStance = True,\n",
    "        batchFirst = True,\n",
    "        dropout = 0.0 # 模型默认使用的drop out概率\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = 'cpu'\n",
    "        self.s2vDim = s2vDim\n",
    "        self.gcnHiddenDim = gcnHiddenDim\n",
    "        self.rumorFeatureDim = rumorFeatureDim\n",
    "        self.numRumorTag = numRumorTag\n",
    "        self.numStanceTag = numStanceTag\n",
    "        self.dropout = dropout\n",
    "        self.batchFirst = batchFirst\n",
    "        self.batchSize = 1 # 实际上，由于不会写支持batch化的GCN，我们把1个thread视作1个batch\n",
    "        self.w2vAttentionHeads = w2vAttentionHeads\n",
    "        self.s2vAttentionHeads = s2vAttentionHeads\n",
    "        self.needStance = needStance\n",
    "\n",
    "        # 使用预训练word2vec初始化embed层的参数\n",
    "        self.w2vDim = word2vec.vector_size\n",
    "        weight = torch.zeros(len(word2index) + 1, self.w2vDim) # 留出0号位置给pad\n",
    "        for i in range(len(word2vec.index_to_key)):\n",
    "            try:\n",
    "                index = word2index[word2vec.index_to_key[i]]\n",
    "            except:\n",
    "                continue\n",
    "            weight[index] = torch.FloatTensor(word2vec[word2vec.index_to_key[i]].tolist())\n",
    "        self.embed = nn.Embedding.from_pretrained(weight, freeze = False, padding_idx = 0)\n",
    "        \n",
    "        # sentence embed模块\n",
    "        self.wordAttention = nn.MultiheadAttention(\n",
    "            embed_dim = self.w2vDim,\n",
    "            num_heads = w2vAttentionHeads,\n",
    "            dropout = dropout,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.s2vFc = nn.Linear(self.w2vDim, self.s2vDim)\n",
    "        \n",
    "        # GCN 谣言检测模块\n",
    "        if needStance:\n",
    "            self.biGCN = BiGCN(2 * self.s2vDim, self.gcnHiddenDim, self.rumorFeatureDim, self.numRumorTag)\n",
    "        else:\n",
    "            self.biGCN = BiGCN(self.s2vDim, self.gcnHiddenDim, self.rumorFeatureDim, self.numRumorTag)\n",
    "        self.RumorFc = nn.Linear((rumorFeatureDim + gcnHiddenDim) * 2, numRumorTag)\n",
    "\n",
    "        # Attention立场分析模块\n",
    "        self.stanceAttention = nn.MultiheadAttention(\n",
    "            embed_dim = self.s2vDim,\n",
    "            num_heads = s2vAttentionHeads,\n",
    "            dropout = dropout,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.stanceFc = nn.Linear(self.s2vDim, numStanceTag)\n",
    "\n",
    "    # 根据输入的任务标识进行前向迭代，\n",
    "    def forward(self, thread):\n",
    "        shape = list(thread['nodeText'].shape)\n",
    "        shape.append(-1)\n",
    "        nodeText = thread['nodeText'].view(-1).to(self.device)\n",
    "        nodeFeature = self.embed(nodeText).view(tuple(shape))\n",
    "        nodeFeature, _ = self.wordAttention(nodeFeature, nodeFeature, nodeFeature)\n",
    "        nodeFeature = torch.tanh(self.s2vFc(nodeFeature))\n",
    "\n",
    "        # stance classification:\n",
    "        nodeFeature, _ = self.stanceAttention(nodeFeature, nodeFeature, nodeFeature)\n",
    "        # 取出<start> token对应的Attention得分作为节点的stance特征\n",
    "        stanceFeature = []\n",
    "        for post in nodeFeature:\n",
    "            stanceFeature.append(post[0])\n",
    "        stanceFeature = torch.stack(stanceFeature, dim = 0)\n",
    "        \n",
    "        # rumor detection:\n",
    "        # 取出<start> token对应的Attention得分作为节点的sentence Embedding\n",
    "        s2v = []\n",
    "        for post in nodeFeature:\n",
    "            s2v.append(post[0])\n",
    "        s2v = torch.stack(s2v, dim = 0)\n",
    "        # rumor detection的预测需要结合stance feature\n",
    "        if self.needStance: \n",
    "            s2v = torch.cat([s2v, stanceFeature], dim = 1) \n",
    "        dataTD = Data(\n",
    "            x = torch.clone(s2v).to(self.device), \n",
    "            edgeIndex = thread['edgeIndexTD'].to(self.device), \n",
    "            rootIndex = thread['threadIndex']\n",
    "        )\n",
    "        dataBU = Data(\n",
    "            x = torch.clone(s2v).to(self.device), \n",
    "            edgeIndex = thread['edgeIndexBU'].to(self.device), \n",
    "            rootIndex = thread['threadIndex']\n",
    "        )\n",
    "        rumorFeature = self.biGCN(dataTD, dataBU)\n",
    "\n",
    "        return self.RumorFc(rumorFeature), self.stanceFc(stanceFeature)\n",
    "\n",
    "    # 更换计算设备\n",
    "    def set_device(self, device: torch.device) -> torch.nn.Module:\n",
    "        _model = self.to(device)\n",
    "        _model.device = device\n",
    "        return _model\n",
    "    # 保存模型\n",
    "    def save(self, path: str):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    # 加载模型\n",
    "    def load(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "# GCN实现\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outDim, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(inputDim, hiddenDim)\n",
    "        self.conv2 = GCNConv(hiddenDim + inputDim, outDim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        posts, edge_index, rootIndex = data.x, data.edgeIndex, data.rootIndex # posts(n, inputDim), edgeIndex(2, |E|)\n",
    "        \n",
    "        conv1Out = self.conv1(posts, edge_index)\n",
    "        postRoot = torch.clone(posts[rootIndex])\n",
    "        postRoot = postRoot.repeat(posts.shape[0], 1)\n",
    "        conv1Root = conv1Out[rootIndex]\n",
    "\n",
    "        conv2In = torch.cat([conv1Out, postRoot], dim=1)\n",
    "        conv2In = F.relu(conv2In)\n",
    "        conv2In = F.dropout(conv2In, training=self.training, p=self.dropout) # BiGCN对于dropout的实现，一次卷积之后随机舍弃一些点\n",
    "        conv2Out = self.conv2(conv2In, edge_index)\n",
    "        conv2Out = F.relu(conv2Out)\n",
    "\n",
    "        conv1Root = conv1Root.repeat(posts.shape[0], 1)\n",
    "        feature = torch.cat([conv1Root, conv2Out], dim=1)\n",
    "        # 使用均值计算，把所有节点的特征聚合成为图的特征\n",
    "        feature = torch.mean(feature, dim=0).view(1, -1)\n",
    "        return feature\n",
    "    \n",
    "    # 更换计算设备\n",
    "    def set_device(self, device: torch.device) -> torch.nn.Module:\n",
    "        _model = self.to(device)\n",
    "        _model.device = device\n",
    "        return _model\n",
    "    # 保存模型\n",
    "    def save(self, path: str):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    # 加载模型\n",
    "    def load(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "# BiGCN\n",
    "class BiGCN(torch.nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, convOutDim, NumRumorTag):\n",
    "        super(BiGCN, self).__init__()\n",
    "        self.TDGCN = GCN(inputDim, hiddenDim, convOutDim)\n",
    "        self.BUGCN = GCN(inputDim, hiddenDim, convOutDim)\n",
    "        self.fc=torch.nn.Linear((convOutDim + hiddenDim) * 2, NumRumorTag)\n",
    "\n",
    "    def forward(self, dataTD, dataBU):\n",
    "        TDOut = self.TDGCN(dataTD)\n",
    "        BUOut = self.BUGCN(dataBU)\n",
    "        feature = torch.cat((TDOut, BUOut), dim=1)\n",
    "        return feature\n",
    "\n",
    "    # 更换计算设备\n",
    "    def set_device(self, device: torch.device) -> torch.nn.Module:\n",
    "        _model = self.to(device)\n",
    "        _model.device = device\n",
    "        return _model\n",
    "    # 保存模型\n",
    "    def save(self, path: str):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    # 加载模型\n",
    "    def load(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53024653744be1c7fdb50f623c21bc7cbc066eeadd60abd98245bb69a9f2fbe7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('xsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
